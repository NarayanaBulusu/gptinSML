# gptinSML
a gpt2 implementation in Standard ML - quite simple theoretically, however, difficult to implement in a functional language like standard ml.basically, logic is that a seq of indicies feed into transformer, then prob dist over next index in seq comes out.

some more imports of libraries are neccecary, but this is based on the content that I was taught as a student in HarrisonGrodin's 15-150 class over summer 24' - Functional Programming - which taught functional programming in the language, Standard ML, which was partially pioneered by CMU professor, Robert Harper. 

after enjoying functional programming quite a bit and reading a book, building LLMs for production by Bouchard and Peters, and working helpful repositories implemeting gpt2 and beyond via pytorch, etc, I decided to implement a transformer model in Standard ML. 

enjoy!
